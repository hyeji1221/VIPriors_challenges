# -*- coding: utf-8 -*-
"""data_augmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HL2EFggjbplEBJiFM6c6cvmnRdy6ssO4

# Data Augmentation

## Annotation 정보

bbox : bounding box

area : bbox에서 맨 뒤 두 요소의 곱 (height*width)

bbox_mode : 1로 고정(coco annotation에서 뭐 설정해주는 것 같음)

category_id : 대상이 공인지 사람인지(그냥 원본에서 가져오면 됨)

segmentation
   -  size : [height, width]의 list 형식
   -  counts : segmentatation mask 인코딩한 결과

file_name : 이미지 이름(원본 이미지 이름 변형해서 써줘야 할 듯)

height : height
width : width

image_id : file_name에서 .png 뺀 거

id : 그냥 annotation 구분용 id인 듯. 하나씩 커지면서 할당해주면 될 듯.

iscrowd : mask의 대상이 crowd인지 여부 (원본 데이터에서 가져다 쓰면 될 듯)
"""
import copy
import os
import cv2
import json
import albumentations as A
from albumentations import HueSaturationValue

import pycocotools.mask as m
import glob
import numpy as np
import matplotlib.pyplot as plt
import random

# Define the directory where the images and JSON files are located
img_dir = './data/train_cpv2'
augmented_img_dir = './data/aug/'
json_dir = './data/train_cpv2.json'
output_dir = './data/aug.json'


# 증강된 이미지간 구분을 위해 aug_num 변수를 항시 신경써주세요
# 다른 증강끼리는 다른 aug_num을 설정해줘야 합니다
aug_num = 7

# json 파일의 annotation 파트 생성
# json 파일의 annotation 파트 생성
def annotation(bbox_new, counts_new, anno_origin, json_data, aug_num, img):
    img_h, img_w = img.shape[0], img.shape[1]
    annotation_ = copy.deepcopy(anno_origin)
    # Convert the RLE data to a JSON-serializable format
    encoded_counts = m.encode(np.asfortranarray(counts_new.astype(np.uint8)))
    decoded_counts = encoded_counts['counts'].decode()

    annotation_['bbox'] = bbox_new
    annotation_['area'] = bbox_new[2] * bbox_new[3]
    annotation_['segmentation'] = encoded_counts
    annotation_['segmentation']['size'] = list([img_h, img_w])
    annotation_['segmentation']['counts'] = decoded_counts
    try:
      m.decode(annotation_['segmentation'])
    except:
      return None
    annotation_['height'] = img_h
    annotation_['width'] = img_w
    annotation_['id'] = 0 + 1
    annotation_['image_id'] = str(anno_origin['image_id']) + '_' + str(aug_num)
    annotation_['file_name'] = annotation_['image_id'] + '.png'

    return annotation_


# json 파일의 images 파트 생성. 위의 annotation 함수의 결과를 파라미터로
# json 파일의 images 파트 생성. 위의 annotation 함수의 결과를 파라미터로
def anno_images(new_annotation):
  annotation_ = dict()

  annotation_['file_name'] = new_annotation['file_name']
  annotation_['height'] = new_annotation['height']
  annotation_['width'] = new_annotation['width']
  annotation_['id'] = new_annotation['image_id']

  return annotation_

# 새로운 annotation과 images 파트 추가한 json 생성. 원본 json과 새로운 annotation, images dictionary 파라미터로
def dump_json(json_origin, new_image_anno, new_anno, flag_images):
  json_origin['annotations'].append(new_anno)
  if flag_images==0:
    json_origin['images'].append(new_image_anno)

  return json_origin


def process_image(img_path, annotations, aug_num):
    img_num = 0
    for i, anno in enumerate(annotations['annotations']):
      if anno['image_id'] in img_path:
        img_num = i
        break

    data = copy.deepcopy(annotations)
    img_id = data['annotations'][img_num]['image_id']
    mask_ = []


    # mask_ 리스트에는 같은 이미지에 대한 모든 mask가 담깁니다.
    for i, datum in enumerate(data['annotations']):
        if img_id == datum['image_id']:
          mask_.append(m.decode(datum['segmentation']))

    bbox = data['annotations'][img_num]['bbox']
    img_name = data['annotations'][img_num]['file_name']
    img = cv2.imread(img_path)

    if img is None:
      print("return")
      return None

    # 이미지 증강 파트. mask/bbox 모두 새로 추출해야 함.
    mask_len = len(mask_)
    original_height, original_width = img.shape[:2]

    aug = A.Compose([
    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5)
])

    if mask_len <= 1:
      print("no mask")
      print()
      return None

    new_annotation = []

    for i in range(mask_len):
      print('mask num'+str(i)+" in progress... : total "+str(mask_len)+" masks")
      augmented = aug(image=img, mask=mask_[i], bbox=bbox)
      image_center_cropped = augmented['image']
      mask_center_cropped = augmented['mask']
      bbox_center_cropped = augmented['bbox']

      # make new annotations
      new_anno_ = annotation(bbox_new=bbox_center_cropped, counts_new=mask_center_cropped, anno_origin=data['annotations'][img_num], json_data=data, aug_num = aug_num, img = img)
      new_annotation.append(new_anno_)

    # make new images annotation
    new_image_anno = anno_images(new_anno_)

    # save img
    new_img_dir = os.path.join(augmented_img_dir, new_annotation[0]['file_name'])
    print()
    if new_annotation[0]['file_name'] in os.listdir(augmented_img_dir):
      print("############################")
      print(new_annotation[0]['file_name'])
      return None
    cv2.imwrite(new_img_dir, image_center_cropped)
    print()

    # make new json data
    flag_images = 0
    for new_anno in new_annotation:
      data = dump_json(data, new_image_anno, new_anno, flag_images)
      flag_images = 1
    start_id = 10000000
    for id in data['annotations']:
      id['id'] = start_id
      start_id += 1

    return data


def process_all_images(img_dir, json_dir, output_dir, aug_num):
    flag = 0
    img_files = os.listdir(img_dir)
    img_len = len(img_files)

    dir_list = os.listdir(img_dir)[:-1]

    for i in range(4):
      with open(json_dir, 'r') as f:
          data = json.load(f)

      for i, img_file in enumerate(img_files):
          if i+1 == len(img_files):
            break
          if flag == 1:
            with open(output_dir, 'r') as f:
                data = json.load(f)
          with open(json_dir, 'r') as f:
            dir_sample = random.choice(dir_list)
            data_ = json.load(f)
          data_target = random.choice(data_['annotations'])
          img_path = os.path.join(img_dir, img_file)

          print('img num'+str(i)+" in progress... : total "+str(img_len)+" imgs")
          json_object = process_image(img_path, data, aug_num)

          if '.png' not in img_path:
            print("no png file")
            return

          if json_object is None:
            continue

          with open(output_dir, 'w') as f:
              json.dump(json_object, f, indent=2)
              flag = 1
      aug_num += 1

# Run the processing for all images in the directory
process_all_images(img_dir, json_dir, output_dir, aug_num)
