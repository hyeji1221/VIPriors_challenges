{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZI5LrYyrH8o",
        "outputId": "ec0f66ff-7ba1-4d44-e502-8b361e746853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os, distutils.core\n",
        "\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "import torch, detectron2\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-WL_ysxrcvO",
        "outputId": "5f47f13d-3d87-4ca7-aa6f-459e75c75010"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'detectron2'...\n",
            "remote: Enumerating objects: 15180, done.\u001b[K\n",
            "remote: Counting objects: 100% (201/201), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 15180 (delta 111), reused 120 (delta 63), pack-reused 14979\u001b[K\n",
            "Receiving objects: 100% (15180/15180), 6.21 MiB | 7.32 MiB/s, done.\n",
            "Resolving deltas: 100% (10973/10973), done.\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (8.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.3)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black\n",
            "  Downloading black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs>=0.1.8) (6.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.56.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.40.0)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.1)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.6)\n",
            "Collecting mypy-extensions>=0.4.3 (from black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black)\n",
            "  Downloading pathspec-0.11.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (3.9.1)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
            "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61405 sha256=6d81cf27484b444d74268bd64dd71f5da21d36ad61430dfa7bf69e8bef3368fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=a32e5304073784286afa0d1a2584998a2f6bc402cb9250ca85537bb5d49a4913\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-23.7.0 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.11.1 portalocker-2.7.0 yacs-0.1.8\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "torch:  2.0 ; cuda:  cu118\n",
            "detectron2: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # # Register DeepSports dataset\n",
        "\n",
        "    # # register_coco_instances(\"deepsports_train\", {}, os.path.join(args.dataset_path,\"train.json\"),\n",
        "    # #                         os.path.join(args.dataset_path,\"train\"))\n",
        "    # # register_coco_instances(\"deepsports_val\", {}, os.path.join(args.dataset_path,\"val.json\"),\n",
        "    # #                         os.path.join(args.dataset_path,\"val\"))\n",
        "    # # register_coco_instances(\"deepsports_test\", {},\n",
        "    # #                         os.path.join(args.dataset_path,\"test_nolabels.json\"),\n",
        "    # #                         os.path.join(args.dataset_path,\"test\"))\n",
        "\n",
        "\n",
        "\n",
        "    # register_coco_instances(\"deepsports_train\", {}, \"/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/train.json\",\n",
        "    #                        \"/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/train\")\n",
        "    # register_coco_instances(\"deepsports_val\", {}, \"/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/val.json\",\n",
        "    #                         \"/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/val\")\n",
        "    # register_coco_instances(\"deepsports_test\", {},\n",
        "    #                         \"/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/test_nolabels.json\",\n",
        "    #                         \"/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/test\")"
      ],
      "metadata": {
        "id": "lsihsa2DlcAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Baseline code for the 2021 VIPriors Instance Segmentation challenge based on detectron2.\n",
        "To use, install detectron2 () and or use the detectron2 default docker image.\n",
        "\n",
        "Usage:\n",
        "- Train baseline model:     python baseline.py --dataset_path 'path/to/dataset'\n",
        "- Predict on test split:    python baseline.py --dataset_path 'path/to/dataset' --predict 'model_weights.pth.tar'\n",
        "\n",
        "Use the last option for generating the prediction files that can be uploaded to CodaLab.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# # Setup detectron2 logger\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# import some common libraries\n",
        "import torch\n",
        "import os, argparse, sys, json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.modeling import build_model\n",
        "import easydict\n",
        "\n",
        "import pycocotools.mask as mask_util\n",
        "\n",
        "setup_logger()\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # Register DeepSports dataset\n",
        "    register_coco_instances(\"deepsports_train\", {}, \"/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/train.json\",\n",
        "                           \"/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/train\")\n",
        "    register_coco_instances(\"deepsports_val\", {}, \"/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/val.json\",\n",
        "                            \"/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/val\")\n",
        "    register_coco_instances(\"deepsports_test\", {},\n",
        "                            \"/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/test_nolabels.json\",\n",
        "                            \"/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/test\")\n",
        "\n",
        "    # Load config file and adjust where needed\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "    cfg.INPUT.MASK_FORMAT = 'bitmask'\n",
        "    cfg.DATASETS.TRAIN = (\"deepsports_train\",)\n",
        "    cfg.DATASETS.TEST = (\"deepsports_val\",)\n",
        "    cfg.DATALOADER.NUM_WORKERS = 2  # 4\n",
        "    cfg.SOLVER.IMS_PER_BATCH = 2 # args.bs\n",
        "    cfg.SOLVER.BASE_LR = args.lr\n",
        "    cfg.SOLVER.MAX_ITER = 1000 # args.iter\n",
        "    cfg.SOLVER.STEPS = []\n",
        "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # human (1), ball (2)\n",
        "    cfg.MODEL.WEIGHTS = None # transfer learning is prohibited!\n",
        "    print(cfg)\n",
        "\n",
        "    # Create output dir\n",
        "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    if args.predict:\n",
        "        # load model and weights\n",
        "        model = build_model(cfg)\n",
        "        DetectionCheckpointer(model).load(args.predict)\n",
        "        # make dataloader\n",
        "        test_loader = build_detection_test_loader(cfg, \"deepsports_test\")\n",
        "        # predict\n",
        "        outputs = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader):\n",
        "                pred = model(batch)\n",
        "                pred = batch_to_dict(pred, batch) # convert pred to serializable format\n",
        "                outputs.extend(pred)\n",
        "\n",
        "        with open('prediction.json', 'w') as fout:\n",
        "            json.dump(outputs, fout)\n",
        "\n",
        "        # exit\n",
        "        sys.exit()\n",
        "\n",
        "    # Train\n",
        "    trainer = DefaultTrainer(cfg)\n",
        "    trainer.resume_or_load(resume=False)\n",
        "    trainer.train()\n",
        "    evaluator = COCOEvaluator(\"deepsports_val\", (\"segm\",), False, output_dir=\"./output/\")\n",
        "    val_loader = build_detection_test_loader(cfg, \"deepsports_val\")\n",
        "    # print(inference_on_dataset(model, val_loader, evaluator))\n"
      ],
      "metadata": {
        "id": "rQ-4_hLzrIlH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_to_dict(pred, batch):\n",
        "    # TODO: check exact format to convert batch into\n",
        "    results_list = []\n",
        "    categories = pred[0]['instances'].pred_classes.cpu().numpy().tolist()\n",
        "    scores = pred[0]['instances'].scores.cpu().numpy().tolist()\n",
        "    masks = pred[0]['instances'].pred_masks.cpu()\n",
        "\n",
        "    rles = [mask_util.encode(np.array(mask[:, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0] for mask in masks]\n",
        "    for rle in rles:\n",
        "        rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "    for i in range(len(pred[0]['instances'])):\n",
        "        results_list.append({'image_id': batch[0]['image_id'],\n",
        "                             'category_id': categories[i]+1,\n",
        "                             'segmentation': rles[i],\n",
        "                             'score': scores[i]})\n",
        "\n",
        "    return results_list"
      ],
      "metadata": {
        "id": "tcuhDS9_lr65"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import easydict\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # parser = argparse.ArgumentParser(description='VIPriors Segmentation baseline training script')\n",
        "    # parser.add_argument('--dataset_path', metavar='path/to/data/root', default='/content/drive/MyDrive/2023 VIPriors Instance Segmentation Challenge/vipriors-segmentation-data-2022/vipriors-segmentation-data-2022',\n",
        "    #                     type=str, help='path to dataset (ends with /data)')\n",
        "    # parser.add_argument('--lr', metavar='1e-4', default=1e-4, type=float, help='learning rate')\n",
        "    # parser.add_argument('--bs', metavar='10', default=10, type=int, help='batch size')\n",
        "    # parser.add_argument('--iter', metavar='10000', default=10000, type=int, help='training iterations')\n",
        "    # parser.add_argument('--predict', default=None, type=str, help='provide model weights to run inference')\n",
        "    # parser.add_argument(\"-f\", required=False)\n",
        "    # args = parser.parse_args()\n",
        "    args = easydict.EasyDict({\n",
        "       \"dataset_path\": '/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022' ,\n",
        "       \"lr\": 1e-4 , #1e-4 -> 0.0001\n",
        "       \"bs\": 10,\n",
        "       \"iter\": 99,\n",
        "       \"predict\": None })\n",
        "\n",
        "\n",
        "    print(args)\n",
        "\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmZQ-7yRltAs",
        "outputId": "ed59569c-3fe4-4233-8ec3-59365d54ae6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dataset_path': '/content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022', 'lr': 0.0001, 'bs': 10, 'iter': 99, 'predict': None}\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 2\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('deepsports_val',)\n",
            "  TRAIN: ('deepsports_train',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: bitmask\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: DefaultAnchorGenerator\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[32], [64], [128], [256], [512]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_resnet_fpn_backbone\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: True\n",
            "  META_ARCHITECTURE: GeneralizedRCNN\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 80\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    FED_LOSS_FREQ_WEIGHT_POWER: 0.5\n",
            "    FED_LOSS_NUM_CLASSES: 50\n",
            "    NAME: FastRCNNConvFCHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 2\n",
            "    POOLER_RESOLUTION: 7\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "    USE_FED_LOSS: False\n",
            "    USE_SIGMOID_CE: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 128\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: StandardROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 2\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 4\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    CONV_DIMS: [-1]\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 1000\n",
            "    PRE_NMS_TOPK_TEST: 1000\n",
            "    PRE_NMS_TOPK_TRAIN: 2000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: None\n",
            "OUTPUT_DIR: ./output\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: False\n",
            "  BASE_LR: 0.0001\n",
            "  BASE_LR_END: 0.0\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 5000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 1.0\n",
            "    ENABLED: False\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 2\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 1000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  NUM_DECAYS: 3\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  RESCALE_INTERVAL: False\n",
            "  STEPS: []\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: None\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 100\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VIS_PERIOD: 0\n",
            "[07/26 04:06:08 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[07/26 04:06:10 d2.data.datasets.coco]: Loading /content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/train.json takes 1.87 seconds.\n",
            "[07/26 04:06:10 d2.data.datasets.coco]: Loaded 184 images in COCO format from /content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/train.json\n",
            "[07/26 04:06:10 d2.data.build]: Removed 8 images with no usable annotations. 176 images left.\n",
            "[07/26 04:06:10 d2.data.build]: Distribution of instances among all 2 categories:\n",
            "|  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|\n",
            "|   human    | 1410         |    ball    | 95           |\n",
            "|            |              |            |              |\n",
            "|   total    | 1505         |            |              |\n",
            "[07/26 04:06:10 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "[07/26 04:06:10 d2.data.build]: Using training sampler TrainingSampler\n",
            "[07/26 04:06:10 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[07/26 04:06:10 d2.data.common]: Serializing 176 elements to byte tensors and concatenating them all ...\n",
            "[07/26 04:06:10 d2.data.common]: Serialized dataset takes 0.85 MiB\n",
            "[07/26 04:06:10 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from None ...\n",
            "[07/26 04:06:10 d2.engine.train_loop]: Starting training from iteration 0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07/26 04:06:41 d2.utils.events]:  eta: 0:16:29  iter: 19  total_loss: 89.11  loss_cls: 26.01  loss_box_reg: 15.77  loss_mask: 2.44  loss_rpn_cls: 12.75  loss_rpn_loc: 28.51    time: 0.9920  last_time: 1.0932  data_time: 0.8328  last_data_time: 0.9449   lr: 1.9981e-06  max_mem: 2421M\n",
            "[07/26 04:07:05 d2.utils.events]:  eta: 0:16:00  iter: 39  total_loss: 10.75  loss_cls: 3.822  loss_box_reg: 1.587  loss_mask: 0.7627  loss_rpn_cls: 1.197  loss_rpn_loc: 3.967    time: 1.0344  last_time: 0.6882  data_time: 0.7661  last_data_time: 0.5278   lr: 3.9961e-06  max_mem: 2421M\n",
            "[07/26 04:07:26 d2.utils.events]:  eta: 0:15:59  iter: 59  total_loss: 7.725  loss_cls: 3.433  loss_box_reg: 0.6119  loss_mask: 0.691  loss_rpn_cls: 0.7381  loss_rpn_loc: 2.215    time: 1.0270  last_time: 1.0810  data_time: 0.8544  last_data_time: 0.9133   lr: 5.9941e-06  max_mem: 2421M\n",
            "[07/26 04:07:47 d2.utils.events]:  eta: 0:15:49  iter: 79  total_loss: 4.922  loss_cls: 1.647  loss_box_reg: 0.3476  loss_mask: 0.6529  loss_rpn_cls: 0.5105  loss_rpn_loc: 1.343    time: 1.0364  last_time: 1.1175  data_time: 0.9115  last_data_time: 0.9853   lr: 7.9921e-06  max_mem: 2421M\n",
            "[07/26 04:08:01 d2.utils.events]:  eta: 0:13:57  iter: 99  total_loss: 3.85  loss_cls: 0.9064  loss_box_reg: 0.2552  loss_mask: 0.639  loss_rpn_cls: 0.5287  loss_rpn_loc: 1.397    time: 0.9701  last_time: 0.6693  data_time: 0.5454  last_data_time: 0.4864   lr: 9.9901e-06  max_mem: 2421M\n",
            "[07/26 04:08:12 d2.utils.events]:  eta: 0:12:42  iter: 119  total_loss: 3.42  loss_cls: 0.7026  loss_box_reg: 0.2385  loss_mask: 0.63  loss_rpn_cls: 0.4965  loss_rpn_loc: 1.292    time: 0.8926  last_time: 1.1240  data_time: 0.3481  last_data_time: 0.9597   lr: 1.1988e-05  max_mem: 2421M\n",
            "[07/26 04:08:22 d2.utils.events]:  eta: 0:11:43  iter: 139  total_loss: 3.303  loss_cls: 0.6673  loss_box_reg: 0.2704  loss_mask: 0.632  loss_rpn_cls: 0.4334  loss_rpn_loc: 1.061    time: 0.8391  last_time: 0.4061  data_time: 0.3665  last_data_time: 0.2638   lr: 1.3986e-05  max_mem: 2421M\n",
            "[07/26 04:08:33 d2.utils.events]:  eta: 0:10:52  iter: 159  total_loss: 2.896  loss_cls: 0.7497  loss_box_reg: 0.2181  loss_mask: 0.6395  loss_rpn_cls: 0.3862  loss_rpn_loc: 0.6408    time: 0.7981  last_time: 0.5504  data_time: 0.3512  last_data_time: 0.4196   lr: 1.5984e-05  max_mem: 2421M\n",
            "[07/26 04:08:43 d2.utils.events]:  eta: 0:09:49  iter: 179  total_loss: 2.231  loss_cls: 0.5652  loss_box_reg: 0.1924  loss_mask: 0.6326  loss_rpn_cls: 0.3163  loss_rpn_loc: 0.4824    time: 0.7664  last_time: 0.6231  data_time: 0.3508  last_data_time: 0.4401   lr: 1.7982e-05  max_mem: 2421M\n",
            "[07/26 04:08:52 d2.utils.events]:  eta: 0:09:08  iter: 199  total_loss: 2.309  loss_cls: 0.4628  loss_box_reg: 0.2711  loss_mask: 0.6221  loss_rpn_cls: 0.3644  loss_rpn_loc: 0.6459    time: 0.7359  last_time: 0.4911  data_time: 0.3045  last_data_time: 0.3302   lr: 1.998e-05  max_mem: 2421M\n",
            "[07/26 04:09:02 d2.utils.events]:  eta: 0:08:41  iter: 219  total_loss: 2.483  loss_cls: 0.4923  loss_box_reg: 0.2785  loss_mask: 0.6141  loss_rpn_cls: 0.3687  loss_rpn_loc: 0.5666    time: 0.7146  last_time: 0.6777  data_time: 0.3417  last_data_time: 0.5428   lr: 2.1978e-05  max_mem: 2421M\n",
            "[07/26 04:09:13 d2.utils.events]:  eta: 0:08:08  iter: 239  total_loss: 2.238  loss_cls: 0.4441  loss_box_reg: 0.2246  loss_mask: 0.6532  loss_rpn_cls: 0.3345  loss_rpn_loc: 0.4091    time: 0.6979  last_time: 0.5597  data_time: 0.3507  last_data_time: 0.3901   lr: 2.3976e-05  max_mem: 2421M\n",
            "[07/26 04:09:23 d2.utils.events]:  eta: 0:07:46  iter: 259  total_loss: 2.648  loss_cls: 0.6422  loss_box_reg: 0.4634  loss_mask: 0.6255  loss_rpn_cls: 0.3564  loss_rpn_loc: 0.4539    time: 0.6824  last_time: 0.5218  data_time: 0.3372  last_data_time: 0.3860   lr: 2.5974e-05  max_mem: 2421M\n",
            "[07/26 04:09:33 d2.utils.events]:  eta: 0:07:32  iter: 279  total_loss: 2.499  loss_cls: 0.5665  loss_box_reg: 0.516  loss_mask: 0.6507  loss_rpn_cls: 0.3555  loss_rpn_loc: 0.3991    time: 0.6717  last_time: 0.7134  data_time: 0.3731  last_data_time: 0.5428   lr: 2.7972e-05  max_mem: 2421M\n",
            "[07/26 04:09:43 d2.utils.events]:  eta: 0:07:06  iter: 299  total_loss: 2.525  loss_cls: 0.5515  loss_box_reg: 0.5649  loss_mask: 0.6058  loss_rpn_cls: 0.3413  loss_rpn_loc: 0.3876    time: 0.6586  last_time: 0.2724  data_time: 0.3091  last_data_time: 0.1326   lr: 2.997e-05  max_mem: 2421M\n",
            "[07/26 04:09:53 d2.utils.events]:  eta: 0:06:49  iter: 319  total_loss: 2.048  loss_cls: 0.4254  loss_box_reg: 0.5603  loss_mask: 0.6013  loss_rpn_cls: 0.305  loss_rpn_loc: 0.2687    time: 0.6502  last_time: 0.5039  data_time: 0.3604  last_data_time: 0.3295   lr: 3.1968e-05  max_mem: 2421M\n",
            "[07/26 04:10:03 d2.utils.events]:  eta: 0:06:32  iter: 339  total_loss: 1.96  loss_cls: 0.4166  loss_box_reg: 0.5025  loss_mask: 0.596  loss_rpn_cls: 0.2702  loss_rpn_loc: 0.2347    time: 0.6401  last_time: 0.3540  data_time: 0.3126  last_data_time: 0.2241   lr: 3.3966e-05  max_mem: 2421M\n",
            "[07/26 04:10:11 d2.utils.events]:  eta: 0:06:15  iter: 359  total_loss: 2.207  loss_cls: 0.4191  loss_box_reg: 0.4337  loss_mask: 0.5974  loss_rpn_cls: 0.3026  loss_rpn_loc: 0.2441    time: 0.6270  last_time: 0.2956  data_time: 0.2518  last_data_time: 0.1401   lr: 3.5964e-05  max_mem: 2421M\n",
            "[07/26 04:10:21 d2.utils.events]:  eta: 0:06:00  iter: 379  total_loss: 2.498  loss_cls: 0.6011  loss_box_reg: 0.5555  loss_mask: 0.6002  loss_rpn_cls: 0.3195  loss_rpn_loc: 0.3101    time: 0.6205  last_time: 0.2311  data_time: 0.3381  last_data_time: 0.0798   lr: 3.7962e-05  max_mem: 2421M\n",
            "[07/26 04:10:31 d2.utils.events]:  eta: 0:05:45  iter: 399  total_loss: 2.215  loss_cls: 0.5168  loss_box_reg: 0.5632  loss_mask: 0.5818  loss_rpn_cls: 0.2542  loss_rpn_loc: 0.2192    time: 0.6136  last_time: 0.3077  data_time: 0.3165  last_data_time: 0.1558   lr: 3.996e-05  max_mem: 2421M\n",
            "[07/26 04:10:41 d2.utils.events]:  eta: 0:05:31  iter: 419  total_loss: 2.403  loss_cls: 0.5391  loss_box_reg: 0.6561  loss_mask: 0.5989  loss_rpn_cls: 0.2745  loss_rpn_loc: 0.2499    time: 0.6087  last_time: 0.7141  data_time: 0.3509  last_data_time: 0.5569   lr: 4.1958e-05  max_mem: 2421M\n",
            "[07/26 04:10:51 d2.utils.events]:  eta: 0:05:18  iter: 439  total_loss: 2.173  loss_cls: 0.4229  loss_box_reg: 0.5218  loss_mask: 0.6144  loss_rpn_cls: 0.2812  loss_rpn_loc: 0.2647    time: 0.6030  last_time: 0.6421  data_time: 0.3223  last_data_time: 0.4777   lr: 4.3956e-05  max_mem: 2421M\n",
            "[07/26 04:11:02 d2.utils.events]:  eta: 0:05:08  iter: 459  total_loss: 2.156  loss_cls: 0.4462  loss_box_reg: 0.5521  loss_mask: 0.5882  loss_rpn_cls: 0.3135  loss_rpn_loc: 0.2339    time: 0.6005  last_time: 0.3364  data_time: 0.3752  last_data_time: 0.1872   lr: 4.5954e-05  max_mem: 2421M\n",
            "[07/26 04:11:11 d2.utils.events]:  eta: 0:04:56  iter: 479  total_loss: 2.213  loss_cls: 0.485  loss_box_reg: 0.6045  loss_mask: 0.6012  loss_rpn_cls: 0.2607  loss_rpn_loc: 0.2087    time: 0.5957  last_time: 0.3105  data_time: 0.3207  last_data_time: 0.1480   lr: 4.7952e-05  max_mem: 2421M\n",
            "[07/26 04:11:22 d2.utils.events]:  eta: 0:04:44  iter: 499  total_loss: 2.399  loss_cls: 0.4679  loss_box_reg: 0.6453  loss_mask: 0.5902  loss_rpn_cls: 0.2771  loss_rpn_loc: 0.2374    time: 0.5929  last_time: 0.5753  data_time: 0.3665  last_data_time: 0.4284   lr: 4.995e-05  max_mem: 2421M\n",
            "[07/26 04:11:32 d2.utils.events]:  eta: 0:04:29  iter: 519  total_loss: 2.097  loss_cls: 0.425  loss_box_reg: 0.6588  loss_mask: 0.5945  loss_rpn_cls: 0.2499  loss_rpn_loc: 0.1762    time: 0.5892  last_time: 0.6501  data_time: 0.3402  last_data_time: 0.4963   lr: 5.1948e-05  max_mem: 2421M\n",
            "[07/26 04:11:41 d2.utils.events]:  eta: 0:04:15  iter: 539  total_loss: 1.891  loss_cls: 0.3649  loss_box_reg: 0.4615  loss_mask: 0.5887  loss_rpn_cls: 0.2097  loss_rpn_loc: 0.1361    time: 0.5838  last_time: 0.4220  data_time: 0.2849  last_data_time: 0.2559   lr: 5.3946e-05  max_mem: 2421M\n",
            "[07/26 04:11:52 d2.utils.events]:  eta: 0:04:04  iter: 559  total_loss: 2.302  loss_cls: 0.5019  loss_box_reg: 0.659  loss_mask: 0.5856  loss_rpn_cls: 0.2534  loss_rpn_loc: 0.2336    time: 0.5828  last_time: 0.6186  data_time: 0.3902  last_data_time: 0.4395   lr: 5.5944e-05  max_mem: 2421M\n",
            "[07/26 04:12:02 d2.utils.events]:  eta: 0:03:52  iter: 579  total_loss: 2.007  loss_cls: 0.4067  loss_box_reg: 0.5558  loss_mask: 0.5805  loss_rpn_cls: 0.2401  loss_rpn_loc: 0.2392    time: 0.5796  last_time: 0.6122  data_time: 0.3244  last_data_time: 0.4436   lr: 5.7942e-05  max_mem: 2421M\n",
            "[07/26 04:12:12 d2.utils.events]:  eta: 0:03:41  iter: 599  total_loss: 2.115  loss_cls: 0.4569  loss_box_reg: 0.6077  loss_mask: 0.6044  loss_rpn_cls: 0.2902  loss_rpn_loc: 0.2207    time: 0.5768  last_time: 0.7262  data_time: 0.3388  last_data_time: 0.5597   lr: 5.994e-05  max_mem: 2421M\n",
            "[07/26 04:12:21 d2.utils.events]:  eta: 0:03:30  iter: 619  total_loss: 2.077  loss_cls: 0.3763  loss_box_reg: 0.6249  loss_mask: 0.5759  loss_rpn_cls: 0.2453  loss_rpn_loc: 0.2139    time: 0.5735  last_time: 0.3325  data_time: 0.3117  last_data_time: 0.1726   lr: 6.1938e-05  max_mem: 2421M\n",
            "[07/26 04:12:32 d2.utils.events]:  eta: 0:03:20  iter: 639  total_loss: 2.17  loss_cls: 0.4362  loss_box_reg: 0.6662  loss_mask: 0.5849  loss_rpn_cls: 0.2283  loss_rpn_loc: 0.2102    time: 0.5725  last_time: 0.6334  data_time: 0.3793  last_data_time: 0.4800   lr: 6.3936e-05  max_mem: 2422M\n",
            "[07/26 04:12:42 d2.utils.events]:  eta: 0:03:09  iter: 659  total_loss: 2.379  loss_cls: 0.4323  loss_box_reg: 0.8364  loss_mask: 0.5888  loss_rpn_cls: 0.2437  loss_rpn_loc: 0.2135    time: 0.5704  last_time: 0.5970  data_time: 0.3388  last_data_time: 0.4408   lr: 6.5934e-05  max_mem: 2422M\n",
            "[07/26 04:12:50 d2.utils.events]:  eta: 0:02:54  iter: 679  total_loss: 1.793  loss_cls: 0.3654  loss_box_reg: 0.5075  loss_mask: 0.5595  loss_rpn_cls: 0.2069  loss_rpn_loc: 0.1494    time: 0.5653  last_time: 0.3685  data_time: 0.2378  last_data_time: 0.2160   lr: 6.7932e-05  max_mem: 2422M\n",
            "[07/26 04:13:00 d2.utils.events]:  eta: 0:02:42  iter: 699  total_loss: 2.1  loss_cls: 0.387  loss_box_reg: 0.7014  loss_mask: 0.5554  loss_rpn_cls: 0.2524  loss_rpn_loc: 0.2333    time: 0.5629  last_time: 0.3396  data_time: 0.3181  last_data_time: 0.2075   lr: 6.993e-05  max_mem: 2422M\n",
            "[07/26 04:13:11 d2.utils.events]:  eta: 0:02:31  iter: 719  total_loss: 1.762  loss_cls: 0.3994  loss_box_reg: 0.5798  loss_mask: 0.56  loss_rpn_cls: 0.183  loss_rpn_loc: 0.1449    time: 0.5622  last_time: 0.5194  data_time: 0.3728  last_data_time: 0.3447   lr: 7.1928e-05  max_mem: 2422M\n",
            "[07/26 04:13:21 d2.utils.events]:  eta: 0:02:20  iter: 739  total_loss: 1.919  loss_cls: 0.3945  loss_box_reg: 0.5939  loss_mask: 0.5519  loss_rpn_cls: 0.1958  loss_rpn_loc: 0.1904    time: 0.5609  last_time: 0.8373  data_time: 0.3513  last_data_time: 0.6397   lr: 7.3926e-05  max_mem: 2422M\n",
            "[07/26 04:13:30 d2.utils.events]:  eta: 0:02:08  iter: 759  total_loss: 1.91  loss_cls: 0.3584  loss_box_reg: 0.5675  loss_mask: 0.5725  loss_rpn_cls: 0.2131  loss_rpn_loc: 0.1646    time: 0.5578  last_time: 0.3174  data_time: 0.2862  last_data_time: 0.1652   lr: 7.5924e-05  max_mem: 2422M\n",
            "[07/26 04:13:40 d2.utils.events]:  eta: 0:01:58  iter: 779  total_loss: 2.287  loss_cls: 0.4824  loss_box_reg: 0.7917  loss_mask: 0.549  loss_rpn_cls: 0.238  loss_rpn_loc: 0.2169    time: 0.5571  last_time: 0.3182  data_time: 0.3677  last_data_time: 0.1371   lr: 7.7922e-05  max_mem: 2422M\n",
            "[07/26 04:13:51 d2.utils.events]:  eta: 0:01:47  iter: 799  total_loss: 2.088  loss_cls: 0.4105  loss_box_reg: 0.5765  loss_mask: 0.551  loss_rpn_cls: 0.2599  loss_rpn_loc: 0.1968    time: 0.5559  last_time: 0.1867  data_time: 0.3433  last_data_time: 0.0145   lr: 7.992e-05  max_mem: 2422M\n",
            "[07/26 04:14:01 d2.utils.events]:  eta: 0:01:36  iter: 819  total_loss: 2.114  loss_cls: 0.4628  loss_box_reg: 0.7496  loss_mask: 0.5548  loss_rpn_cls: 0.2015  loss_rpn_loc: 0.1936    time: 0.5549  last_time: 0.4206  data_time: 0.3557  last_data_time: 0.2786   lr: 8.1918e-05  max_mem: 2422M\n",
            "[07/26 04:14:12 d2.utils.events]:  eta: 0:01:25  iter: 839  total_loss: 2.051  loss_cls: 0.4183  loss_box_reg: 0.7652  loss_mask: 0.5546  loss_rpn_cls: 0.2122  loss_rpn_loc: 0.1806    time: 0.5545  last_time: 0.4841  data_time: 0.3785  last_data_time: 0.2932   lr: 8.3916e-05  max_mem: 2422M\n",
            "[07/26 04:14:21 d2.utils.events]:  eta: 0:01:14  iter: 859  total_loss: 2.043  loss_cls: 0.4294  loss_box_reg: 0.6817  loss_mask: 0.5329  loss_rpn_cls: 0.2069  loss_rpn_loc: 0.2127    time: 0.5520  last_time: 0.5648  data_time: 0.2796  last_data_time: 0.3843   lr: 8.5914e-05  max_mem: 2422M\n",
            "[07/26 04:14:30 d2.utils.events]:  eta: 0:01:03  iter: 879  total_loss: 2.026  loss_cls: 0.4314  loss_box_reg: 0.6955  loss_mask: 0.5466  loss_rpn_cls: 0.1994  loss_rpn_loc: 0.1776    time: 0.5497  last_time: 0.2173  data_time: 0.2883  last_data_time: 0.0829   lr: 8.7912e-05  max_mem: 2422M\n",
            "[07/26 04:14:40 d2.utils.events]:  eta: 0:00:53  iter: 899  total_loss: 2.065  loss_cls: 0.4402  loss_box_reg: 0.7922  loss_mask: 0.5378  loss_rpn_cls: 0.1733  loss_rpn_loc: 0.1653    time: 0.5486  last_time: 0.7270  data_time: 0.3291  last_data_time: 0.5449   lr: 8.991e-05  max_mem: 2422M\n",
            "[07/26 04:14:48 d2.utils.events]:  eta: 0:00:41  iter: 919  total_loss: 1.9  loss_cls: 0.3841  loss_box_reg: 0.6296  loss_mask: 0.5343  loss_rpn_cls: 0.2118  loss_rpn_loc: 0.1412    time: 0.5462  last_time: 0.2984  data_time: 0.2770  last_data_time: 0.1207   lr: 9.1908e-05  max_mem: 2422M\n",
            "[07/26 04:14:59 d2.utils.events]:  eta: 0:00:31  iter: 939  total_loss: 1.932  loss_cls: 0.3927  loss_box_reg: 0.6187  loss_mask: 0.5463  loss_rpn_cls: 0.1731  loss_rpn_loc: 0.1387    time: 0.5454  last_time: 0.1842  data_time: 0.3464  last_data_time: 0.0080   lr: 9.3906e-05  max_mem: 2422M\n",
            "[07/26 04:15:09 d2.utils.events]:  eta: 0:00:20  iter: 959  total_loss: 2.366  loss_cls: 0.4796  loss_box_reg: 0.8214  loss_mask: 0.5205  loss_rpn_cls: 0.232  loss_rpn_loc: 0.2582    time: 0.5450  last_time: 0.4465  data_time: 0.3633  last_data_time: 0.3031   lr: 9.5904e-05  max_mem: 2422M\n",
            "[07/26 04:15:18 d2.utils.events]:  eta: 0:00:10  iter: 979  total_loss: 1.721  loss_cls: 0.3437  loss_box_reg: 0.6314  loss_mask: 0.5251  loss_rpn_cls: 0.1791  loss_rpn_loc: 0.1282    time: 0.5431  last_time: 0.5816  data_time: 0.2965  last_data_time: 0.3994   lr: 9.7902e-05  max_mem: 2422M\n",
            "[07/26 04:15:28 d2.utils.events]:  eta: 0:00:00  iter: 999  total_loss: 1.972  loss_cls: 0.4349  loss_box_reg: 0.6013  loss_mask: 0.562  loss_rpn_cls: 0.2384  loss_rpn_loc: 0.1424    time: 0.5410  last_time: 0.6133  data_time: 0.2718  last_data_time: 0.4607   lr: 9.99e-05  max_mem: 2422M\n",
            "[07/26 04:15:29 d2.engine.hooks]: Overall training speed: 998 iterations in 0:08:59 (0.5410 s / it)\n",
            "[07/26 04:15:29 d2.engine.hooks]: Total training time: 0:09:06 (0:00:06 on hooks)\n",
            "[07/26 04:15:30 d2.data.datasets.coco]: Loading /content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/val.json takes 1.30 seconds.\n",
            "[07/26 04:15:30 d2.data.datasets.coco]: Loaded 62 images in COCO format from /content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/val.json\n",
            "[07/26 04:15:30 d2.data.build]: Distribution of instances among all 2 categories:\n",
            "|  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|\n",
            "|   human    | 487          |    ball    | 33           |\n",
            "|            |              |            |              |\n",
            "|   total    | 520          |            |              |\n",
            "[07/26 04:15:30 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[07/26 04:15:30 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[07/26 04:15:30 d2.data.common]: Serializing 62 elements to byte tensors and concatenating them all ...\n",
            "[07/26 04:15:30 d2.data.common]: Serialized dataset takes 0.23 MiB\n",
            "WARNING [07/26 04:15:30 d2.engine.defaults]: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
            "[07/26 04:15:30 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[07/26 04:15:30 d2.data.datasets.coco]: Loaded 62 images in COCO format from /content/drive/MyDrive/Colab Notebooks/PR_Lab/2023_study/vipriors-segmentation-data-2022/val.json\n",
            "[07/26 04:15:30 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[07/26 04:15:30 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[07/26 04:15:30 d2.data.common]: Serializing 62 elements to byte tensors and concatenating them all ...\n",
            "[07/26 04:15:30 d2.data.common]: Serialized dataset takes 0.23 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1VGsYIDHltDD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
